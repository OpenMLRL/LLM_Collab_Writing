# Pipeline Evaluation Configuration for TLDR (Summarization)
#
# Pipeline: Agent 1 generates first, Agent 2 receives Agent 1's output in prompt
# Key difference from Parallel: Agent 2 sees Agent 1's concise summary and expands it

config_name: pipeline

# Model Configuration
model:
  name: "Qwen/Qwen3-1.7B"
  type: "qwen"
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "float16"

# Dataset Configuration
dataset:
  name: "trl-lib/tldr"
  type: "tldr"
  eval_split: "test[:100]"  # Subset for quick testing; use "test[:1100]" for full evaluation

# Evaluation Parameters
evaluation:
  num_attempts: 1  # TLDR doesn't use Pass@k, so 1 is sufficient
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  top_k: null

# Output Configuration
output:
  base_dir: "evals/results"
  verbose: true
  save_csv: true
  save_json: true

# Logging
logging:
  wandb_project: "comlrl-evals"
  run_name_prefix: "tldr_pipeline"

# Reward Configuration (for reference)
# TLDR uses 4-level reward:
# - Level 1 (Structural): Both completions 8-256 tokens (max +0.5)
# - Level 2 (Coordination): Length ratio C2/C1 in 1.6-3.2x range (max +1.0)
# - Level 3 (Vocabulary): Unique words ratio C2/C1 >= 2.0x (max +0.5)
# - Level 4 (Style): Transition words + Jaccard similarity (max +1.0)
# Total max: 3.0
