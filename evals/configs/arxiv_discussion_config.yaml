# Discussion (2-turn) Evaluation Configuration for arXiv
# This configuration mirrors the training config for fair comparison

# Model configuration
model:
  name: "Qwen/Qwen3-1.7B"
  type: "qwen"
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "auto"

# Tokenizer settings
tokenizer:
  padding_side: "left"

# Dataset configuration
dataset:
  name: "OpenMLRL/arXiv_abstract"
  type: "arxiv"
  eval_split: "val[:1100]"

# Evaluation settings
evaluation:
  num_attempts: 1
  num_turns: 2  # Turn 1: Parallel, Turn 2: Refinement
  max_new_tokens: 256  # Each agent gets 256 tokens per turn (target 128-256)
  temperature: 0.7
  top_p: 0.9
  top_k: null

# Output configuration
output:
  base_dir: "evals/results"
  verbose: true

# Metrics to collect
metrics:
  - time_per_agent      # Wall-clock generation time (seconds)
  - tokens_per_agent    # Number of tokens produced
  - score               # Total reward from arxiv_combined_reward (max 3.0)
  - level1_reward       # Structural (both 128-256 tokens)
  - level2_reward       # Coordination (length ratio 0.8-1.5x, optimal 1.0-1.3x)
  - level3_reward       # Vocabulary Diversity (unique words ratio 0.7-1.3x)
  - level4_reward       # Style (transition words + Jaccard similarity)
  - length_ratio        # C2/C1 length ratio
  - unique_words_ratio  # Unique words in C2 / C1

# Notes:
# - Discussion configuration: 2-turn evaluation
# - Turn 1 (Parallel): Both agents generate independently
#   - Agent 1: Background and motivation
#   - Agent 2: Methodology and implications
# - Turn 2 (Refinement): Both agents see each other's Turn 1 outputs and refine
#   - Agent 1: Refined background based on complementary section
#   - Agent 2: Refined methodology based on background section
# - Final score from Turn 2 outputs
# - Time: max(T1_A1, T1_A2) + max(T2_A1, T2_A2)
# - Cost: All 4 generations (2 agents x 2 turns)
